tic

f = 0.70; % Fraction of data for training set

% Build multinomial LR classification models and model selection for star ratings of
% restaurant businesses.
%We select some of the features in our dataset and build two competing
%models
%Model 1: relies only on numerical features
%Model 2: includes categorical features as well such as business attributes
%Goal: predict star ratings given the features in our dataset

%Model 1: is based on a small subset of features that have the highest
%correlation with the variable we're trying to predict
%Model 2: includes a more comprehensive dataset and uses PCA for
%dimensionality reduction

%We will compare these models to understand which one does best in
%classifyign star ratings

T=readtable("final_app1.csv",'PreserveVariableNames',true); %Load csv file 3083x18 matrix
%select most relevant columns for analysis



star_ratings=categorical(table2array(T(:,20)));

%First set of predictors
var1=[table2array(T(:,1:2),'VariableNames',{'review_count' 'number_categories'}) table2array(T(:,4:17),'VariableNames',{'avg_stars_user' 'avg_openinghrs' 'Cuisine' 'city' 'Caters' 'AcceptsCreditCards' 'HasTV' 'NoiseLevel' 'WheelchairAcces' 'GoodForGroups' 'Take_out' 'Wi_Fi' 'OutdoorSeating' 'PriceRange' 'Alcohol' 'WaiterService'})];

var2=[table2array(T(:,1:3)) table2array(T(:,5:17))];

%List of star ratings
stars=unique(star_ratings); %9 categories of unique values 

%Building training/testing sets. In order for all ratings to be represented
%with the same proportions as in the whole dataset we assign a fraction f
%of the business in each rating category to the training set and a fraction
%1-f to the test set. 
training_ind=[]; testing_ind=[];


for i =1: length(stars)
    ind=find(star_ratings==stars(i)); %Finding all indices corresponding to the star rating i
    training_ind=[training_ind;  ind(1:round(f*length(ind)))]; %Assigning a fraction f of business that belong to star rating i to the training set. First 75% corresponding to star rating i
    testing_ind=[testing_ind; ind(round(f*length(ind))+1:end)]; %Assigning complementary fraction to the test set. Store the remaining one to the test set
end 

%Training/testing set for model 1

var1_train=var1(training_ind,:);
var1_test=var1(testing_ind,:);

var2_train=var2(training_ind,:);
var2_test=var2(testing_ind,:);


%Training/testing set for dependent variable (stars)
stars_train=star_ratings(training_ind);
stars_test=star_ratings(testing_ind);

Y_train=categorical(stars_train);
Y_test=categorical(stars_test);


%% Calibrating a multinomial LR model for Model 1 (Simple Model)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Model 1 (Simple Model) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Calibrating a Multinomial LR for model 1. B1 is an (M+1)*(K-1) matrix
%where each columns contains the model coefficients for class k=1,...K-1. M
%corresponds to the number of regressors in the model

B1=mnrfit(var1_train, Y_train); %we provide the training features and the training labels . this gives the matrix of coefficients for model 1. 5 star rating is taken as the pivot class
%and it computes beta coefficients for the rest.


%% In sample Likelihood Analysis (Model 1)
%Computing models' likelihood as the product of their probabilities for
%observed values. 
K=length(stars); %Number of classes
N=size(var1_train,1); %Number of items in training set

%Computing in-sample probabilities assigned by model 1. In the following
%line we construct an auxilliary matrix of size N*(K-1) such that item
%(i,k) is the scalar product of vector b_k and vector f_1 where b_k is the
%vector of size M+1 with the model's coefficients for class k, while f_i is
%the vector [1xi] with the features of item i

aux=[ones(N,1) var1_train]*B1;
aux=exp(aux);
pK=1./(1+sum(aux')); %probability that objects in the training set belong 
%to the pivot class K

%In the following two steps we compute an aux matrix containing the
%denominators needed for the probabilities, then multiply them by the above
%aux matrix to get the full probabilities for K-1 classes
aux_pK=(ones(K-1,N).*pK)';
probs1=[aux.*aux_pK pK']; %N*K matrix of probabilities (each row contains the probab
%that a restaurant is rated a given star rating)

% sum(probs1,2) double check tht probs1 has been calculated correctly 

%Computing the in-sample log likelihood for Model 1

L1=0;

for k=1:K  %find all business belonging to a particular star rating
    ind=find(stars_train==stars(k)); %sum log prob of observing the data we have if they were generated by the model. What's the probability that the model assigns to correct genres of songs
    L1=L1+sum(log(probs1(ind,k)));
end 

fprintf('In-sample log-likelihood of Model 1 = %4.3e\n',L1) %negative number because it's negative.best model will be the one that gives larges log-likelihood
%observe the difference and order of magnitude too. The in-sample analysis
%suggests that In-sampel Model 1 is the one that best recaptures the star
%ratings in sample

%% PCA for numerical features (Model 2)

%Find the principal components for the training data set of numerical features.
%This code returns four outputs: coeff, scoreTrain, explained, and mu. 
%Use explained (percentage of total variance explained) to find the number of components required to explain at least 95% variability. 
%Use coeff (principal component coefficients) and mu (estimated means of XTrain) to apply the PCA to a test data set. 
%Use scoreTrain (principal component scores) instead of XTrain when you train a model.
%First standardise the data



%Train a multinomial LR using the first two components of the numerical
%data and the most important categorical data based on correlation analysis


%% Out-of-sample analysis Model 1
K=length(stars); %Number of classes
N=size(var1_test,1); %Number of items in test set

aux=[ones(N,1) var1_test]*B1;
aux=exp(aux);
pK=1./(1+sum(aux')); %probability that objects in the test set belong 
%to the pivot class K

%Computing the OOS probabilities(repeating the exact same steps as before)
aux_pK=(ones(K-1,N).*pK)';
probs1=[aux.*aux_pK pK']; %N*K matrix of probabilities (each row contains the probab
%that a restaurant is rated a given star rating)

%% Evaluating Out of Sample Accuracy Model 1
% Computing star rating predicted by subjective model for each business.
% This is done by assuming the predicted rating is the one associated with
% the highest probability.

m1=max(probs1,[],2); %Finding highest value in each row of probab matrix of the model
ind1=probs1==m1; %Constructing a matrix where one indicates the positive of the max value
[i,j]= find(ind1); %Finding indices for the max values
[i, idx]=sort(i); %Sorting row indices
pred1=j(idx);

%%% Computing accuracy of subjective model as fraction of songs correctly 
%%% assigned to their genre
acc1 = length(find(stars(pred1) == stars_test))/length(stars_test);
fprintf('Out-of-sample accuracy of Model 1 = %4.3f\n',acc1) 

%Note:Out of sample Model 1 has a 30% accuracy. Notion of accuracy is based
%on the assumption that the predicted genre is the one with the highest
%predictive probability

%% Model Intepretation 
%https://uk.mathworks.com/help/stats/confusionchart.html
%C = confusionmat(stars_test,stars(pred1));

%cm = confusionchart(stars_test,stars(pred1))

Mdl = fitctree(var1_train,Y_train,'PredictorSelection','curvature',...
    'Surrogate','on');

imp = predictorImportance(Mdl);

figure;
bar(imp);
title('Predictor Importance Estimates');
ylabel('Estimates');
xlabel('Predictors');
h = gca;
h.XTickLabel = Mdl.PredictorNames;
h.XTickLabelRotation = 45;
h.TickLabelInterpreter = 'none';

%%  Calibrating a multinomial LR model for Model 2 
X_train=var2_train;
%X_train = [scoreTrain(:,1:ncomp) cat_var_train];
%X_train=scoreTrain(:,1:ncomp);
B2=mnrfit(var2_train, Y_train); %we provide the training features and the training labels . this gives the matrix of coefficients for model 1. 5 star rating is taken as the pivot class

%% In sample Likelihood Analysis (Model 2)
%Computing models' likelihood as the product of their probabilities for
%observed values. 
K=length(stars); %Number of classes
N=size(X_train,1); %Number of items in training set

%Computing in-sample probabilities assigned by model 1. In the following
%line we construct an auxilliary matrix of size N*(K-1) such that item
%(i,k) is the scalar product of vector b_k and vector f_1 where b_k is the
%vector of size M+1 with the model's coefficients for class k, while f_i is
%the vector [1xi] with the features of item i

aux=[ones(N,1) X_train]*B2;
aux=exp(aux);
pK=1./(1+sum(aux')); %probability that objects in the training set belong 
%to the pivot class K

%In the following two steps we compute an aux matrix containing the
%denominators needed for the probabilities, then multiply them by the above
%aux matrix to get the full probabilities for K-1 classes
aux_pK=(ones(K-1,N).*pK)';
probs2=[aux.*aux_pK pK']; %N*K matrix of probabilities (each row contains the probab
%that a restaurant is rated a given star rating)

% sum(probs1,2) double check tht probs1 has been calculated correctly 

%Computing the in-sample log likelihood for Model 1

L2=0;

for k=1:K  %find all business belonging to a particular star rating
    ind=find(stars_train==stars(k)); %sum log prob of observing the data we have if they were generated by the model. What's the probability that the model assigns to correct genres of songs
    L2=L2+sum(log(probs2(ind,k)));
end 

fprintf('In-sample log-likelihood of Model 2 (PCA) = %4.3e\n',L2) %negative number because it's negative.best model will be the one that gives larges log-likelihood
%observe the difference and order of magnitude too. The in-sample analysis
%suggests that In-sampel Model 1 is the one that best recaptures the star
%ratings in sample
%% Out-of-sample analysis Model 2
K=length(stars); %Number of classes

%To use the trained model for the test set, you need to transform the test data set by using the PCA obtained from the training data set. 
%Obtain the principal component scores of the test data set by subtracting mu from XTest and multiplying by coeff. 
%Only the scores for the first two components are necessary, so use the first two coefficients coeff(:,1:idx)
%scoreTest = (var2_test-mu)*coeff(:,1:ncomp);
%scoreTest = (var2_test-mu)*coeff(:,1:ncomp);
scoreTest=var2_test;
%var2_test=[scoreTest cat_var_test];
N=size(scoreTest,1); %Number of items in test set

aux=[ones(N,1) scoreTest]*B2;
aux=exp(aux);
pK=1./(1+sum(aux')); %probability that objects in the test set belong 
%to the pivot class K

%Computing the OOS probabilities(repeating the exact same steps as before)
aux_pK=(ones(K-1,N).*pK)';
probs2=[aux.*aux_pK pK']; %N*K matrix of probabilities (each row contains the probab
%that a restaurant is rated a given star rating)

%% Evaluating Out of Sample Accuracy Model 2
% Computing star rating predicted by subjective model for each business.
% This is done by assuming the predicted rating is the one associated with
% the highest probability.

m2=max(probs2,[],2); %Finding highest value in each row of probab matrix of the model
ind2=probs2==m2; %Constructing a matrix where one indicates the positive of the max value
%writematrix(ind2,'Indices.csv') 
%preds2=readtable("preds2.csv");
%pred2=table2array(preds2);
[i2,j2] = find(ind2); %Finding indices for the max values
        
[i2, idx2]=sort(i2); %Sorting row indices
pred2=j2(idx2);

%%% Computing accuracy of subjective model as fraction of songs correctly 
%%% assigned to their genre
acc2 = length(find(stars(pred2) == stars_test))/length(stars_test);
fprintf('Out-of-sample accuracy of Model 2 = %4.3f\n',acc2) 

%Note:Out of sample Model 1 has a 30% accuracy. Notion of accuracy is based
%on the assumption that the predicted genre is the one with the highest
%predictive probability

%% Building ROC curve for each genre

%Again we use the notion that the predicted genre is the one with the
%highest predictive probabaility to measure how many objects are assigned
%to the corresponding class. 

x = linspace(0,1,500);
AUC1=[];
AUC2=[];

for k = 1:K

    subplot(3,5,k)
    
    [X,Y,T,AUC1] = perfcurve(stars_test,probs1(:,k),stars(k));
   
    plot(X,Y,'r-','linewidth',1.5)
    hold on
    
    [X,Y,T,AUC2] = perfcurve(stars_test,probs2(:,k),stars(k));

    
    plot(X,Y,'b-','linewidth',1.5)
    hold on    
    
    plot(x,x,'--k','linewidth',1.5)
    AUC1=[AUC1; AUC1];
    %AUC2=[AUC2; AUC2];

    fprintf('Star Rating: %s; AUC Model 1 = %4.3f; AUC PCA = %4.3f\n',stars(k),AUC1,AUC2)
    title(stars(k))
    
    
end

fprintf('\n')


fprintf('\n')
    
%%% Precision, recall and F1 score for each genre

for k = 1:K
   
    % True positives (number of songs in class correctly classified) for
    % subjective model
    ind1 = find(stars_test == stars(k));
    ind2 = find(stars(pred1) == stars(k));
    
    TP1 = length(intersect(ind1,ind2));
    
    % False positives (number of songs identified wrongly as belonging to
    % class k) for subjective model
    FP1 = length(ind2) - TP1;
    
    % False negatives (number of songs belonging to class k assigned to
    % another class) for subjective model
    FN1 = length(ind1) - TP1;
    
    % Precision (fraction of positives that are true positives) for
    % subjective model
    precision1 = TP1/(TP1+FP1);
    
    % Recall (fraction of objects in class correctly classified) for
    % subjective model
    recall1 = TP1/(TP1+FN1);
    
    % F1 score
    F1score1 = 2*TP1/(2*TP1+FP1+FN1);
    fprintf('Star Ratings: %s; F1 score subj. model = %4.3f; F1 score obj. model = %4.3f\n',stars(k),F1score1)
    
end



fprintf('\n')

% Comment on the results 
% Visually compare the performance of two models on different classes.
% Identify genres where one model significantly outperforms the others
% Take home message: No model is conclusively better; but in certain genres
% Model 2 outperofrms model 1 in several classes.
% We look at F1 scores too for different classes too. We find zero F1 score
% for some classes, these shows that the model do not predict any business
% to belong to the particular class. F1 score of zero but non-zero AUC. The
% difference is that F1 score depends on the value of a threshold. an F1
% score of zero means that across our sample in the testing set, none have
% the highest probability corresponding to that genre. On the other hand
% you have an ROC curve because each song will have an assigned probability
% that corresponds to the genre. so by low enough thresholds you will start
% 


toc

